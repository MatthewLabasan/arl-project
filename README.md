# arl-project
ARL Internship Project

## 6/20/24
Today I finished setting up my environment and got my first node.js server set up with Express.js.
It consists of a basic html file. In my JS file, I set up a get method and allowed my app to listen to requests on port 3000. Today, I also experimented a lot with git
and learned more about coding in javascript. Tomorrow, I want to explore the web scraping side, learn how I can structure my project and backend algorithms, and learn about deployment to a public link.

## 6/21/24
Instead of starting on web scraping exploration, I decided to go into more depth on setting up my node.js server and understanding the connections between the 
backend and frontned. I learned a lot about middlewear, error handling, and how the server is able to retrieve files. I also learned about cors policy and how to handle that, and the implementation of logs. I'm watching a MERN app tutorial, and am following along on another repo. Chatgpt has been really helpful in explaining concepts that aren't fully touched in the video, but I've also been double checking the documentation alongside it. This weekend, I hope to continue this and go into databases (specifically mongo), and some frontend react stuff. By next week, I should be ready to start looking into web scraping and beginning that process on a server that I build myself. Below is a rough timeline I hope to follow:
    - 6/26: Begin web scrapping exploration
    - 7/3: Finish up web scrapping, work on auto emailing / pdf creation of scrapped data,
    - 7/10: Work on database search functionality 
    - 7/17: Work on frontend/fix bugs
    - 7/24: Work on paper and presentation

## 6/26/24
Today, I continued with my tutorials and implemented my MongoDB database into my test project, which was super exciting, and not as bad as I thought it would be! I read a couple chapters on MongoDB design choices, and learned about embedded vs normalized data methods (in both relational and document based), and how we can implement a form of either in MongoDB based on our accessing/writing needs. I am not sure what I would like to do with my web scraping data yet, since I am unsure of the format it will be in. Alongside this, I also learned about controllers and how they work alongside routers and the database, and solidified a general understanding of the MVC framework for the backend of a project. Iâ€™m pretty happy with where I am at right now of in terms of understanding! Tomorrow, I will finish up the small portion I have left on Controllers. After that, I think I will have enough knowledge to move onto webscrapers and I will delve into the other misc. backend and front end things after I learn how to use puppeteer first. 

## 6/27/24
Today, I finished up the controllers part of my tutorial and learned some Puppeteer basics. I also worked on a software architecture diagram that shows the connections and framework of my application. Tomorrow, I will continue looking into Puppeteer and hopefully get into modifying the data that I receive for storage in my database. After this, I plan to learn about some frontend basics as I am unclear on how the backend and frontend servers connect.
